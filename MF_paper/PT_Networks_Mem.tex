%% ****** Start of file apstemplate.tex ****** %
%%
%%
%%   This file is part of the APS files in the REVTeX 4 distribution.
%%   Version 4.1r of REVTeX, August 2010
%%
%%
%%   Copyright (c) 2001, 2009, 2010 The American Physical Society.
%%
%%   See the REVTeX 4 README file for restrictions and more information.
%%
%
% This is a template for producing manuscripts for use with REVTEX 4.0
% Copy this file to another name and then work on that file.
% That way, you always have this original template file to use.
%
% Group addresses by affiliation; use superscriptaddress for long
% author lists, or if there are many overlapping affiliations.
% For Phys. Rev. appearance, change preprint to twocolumn.
% Choose pra, prb, prc, prd, pre, prl, prstab, prstper, or rmp for journal
%  Add 'draft' option to mark overfull boxes with black boxes
%  Add 'showpacs' option to make PACS codes appear
%  Add 'showkeys' option to make keywords appear
\documentclass[aps,prl,preprint,groupedaddress]{revtex4-1}
%\documentclass[aps,prl,preprint,superscriptaddress]{revtex4-1}
%\documentclass[aps,prl,reprint,groupedaddress]{revtex4-1}

% You should use BibTeX and apsrev.bst for references
% Choosing a journal automatically selects the correct APS
% BibTeX style file (bst file), so only uncomment the line
% below if necessary.
%\bibliographystyle{apsrev4-1}

\begin{document}

% Use the \preprint command to place your local institutional report
% number in the upper righthand corner of the title page in preprint mode.
% Multiple \preprint commands are allowed.
% Use the 'preprintnumbers' class option to override journal defaults
% to display numbers if necessary
%\preprint{}

%Title of paper
\title{Phase Transitions in Networks of Memristive Elements}

% repeat the \author .. \affiliation  etc. as needed
% \email, \thanks, \homepage, \altaffiliation all apply to the current
% author. Explanatory text should go in the []'s, actual e-mail
% address or url should go in the {}'s for \email and \homepage.
% Please use the appropriate macro foreach each type of information

% \affiliation command applies to all authors since the last
% \affiliation command. The \affiliation command should follow the
% other information
% \affiliation can be followed by \email, \homepage, \thanks as well.
\author{Forrest C. Sheldon}
\email[]{fsheldon@ucsd.edu}
%\homepage[]{Your web page}
%\thanks{}
%\altaffiliation{}
\affiliation{UCSD}

\author{Massimiliano Di Ventra}
\email[]{diventra@ucsd.edu}
%\homepage[]{Your web page}
%\thanks{}
%\altaffiliation{}
\affiliation{UCSD}

%Collaboration name if desired (requires use of superscriptaddress
%option in \documentclass). \noaffiliation is required (may also be
%used with the \author command).
%\collaboration can be followed by \email, \homepage, \thanks as well.
%\collaboration{}
%\noaffiliation

\date{\today}

\begin{abstract}
% insert abstract here
\end{abstract}

% insert suggested PACS numbers in braces on next line
\pacs{}
% insert suggested keywords - APS authors don't need to do this
%\keywords{}

%\maketitle must follow title, authors, abstract, \pacs, and \keywords
\maketitle

% body of paper here - Use proper section commands
% References should be done using the \cite, \ref, and \label commands
%\section{}
% Put \label in argument of \section for cross-referencing
%\section{\label{}}
%\subsection{}
%\subsubsection{}

\section{INTRODUCTION}

The impending limits of our current computational paradigm have galvanized
interest in alternative forms of computation. Of particular interest to the
authors are attempts to address the von Neumann Bottleneck \cite{Backus1978}
whereby increases in processor speed are muted by the necessity of
shuttling data and addresses back and forth between the processor and memory.
This necessity of the von Neumann architecture suggests a solution in which
computation is performed directly in memory.  Physically coupled memory units
whose dynamics drive the system to the solution of problem
would conceivably circumvent this (or at least translate it into the
problem of transmitting information across the system's extent) but the
realization of such an architecture is a foreboding problem.

In this however, we are buoyed by the fact that the nervous system seems to
compute in just this manner; we perceive no separate logical and memory
faculties that information is shuttled between in the brain.  Memory seems
to be stored
in the strength of synaptic coupling between neurons and through the neuronal
dynamics, the synaptic couplings are modified and computation is performed.
The desire to mimic these properties in hardware is an active field of
research in which memristors have begun to play a significant role.  
Memristor based associative memories \cite{Pershin2010, Eryilmaz2014},
 neural networks, cellular automata,
and other architectures are being explored as ways of realizing some
of the computational abilities of biological neural systems
\textit{in silico}.

In most of these, some abstracted model/element of
biological neural computation is adopted, such as the hopfield network or the
perceptron, and these are then constructed to form a circuit which
performs the desired function.  This design philosophy is termed 'top-down',
where neuromorphic circuits are engineered to accomplish some specific 
function.  The resulting circuits however, do not
much resemble biological neural networks in which disorder and noise are
prevalent.  A complementary approach to neuromorphic engineering is 'bottom-up'
design, in which systems that duplicate the gross features of neural tissue
are constructed and their dynamics studied to understand the role of these
features in creating interesting behaviors.  We can view this as a physical
analogy to studies of cellular automata. In these, sets of rules were examined
to determine what dynamics they produced and whether those dynamics could
support computation.\cite{Langton1990}  We instead examine the dynamics of
physical systems and search for phenomena that may support computation.

While it may seem that this leaves 'top-down' design overly open-ended, the
challenge of fabricating electrical systems with complex, disordered
architecture and interesting dynamics leaves few viable options.  Recently, by
utilizing a growth process, Gimzewskii et al. were able to construct
complex networks
of silver wires that self-assembed from  solution.  After exposure to sulfur
gas, junctions
between silver formations formed atomic switches, capable of bipolar resistive
switching and duplicating many of the behaviors of biological synapses.  They
have proposed this as a minimal hardware implementation neuropil, the
synaptically dense areas of the brain populated by unmyelinated axons.
Experiment has shown a variety of resistive switching and fluctuation
behaviors which may be of interest.  Following the analogy with cellular
automata, we focus on the potential for phase transitions in these systems,
near which their computational capacity may be maximal.  As the experimental
networks are subjected to an AC voltage, a sharp transition occurs between
the insulating and conducting states of the network, causing it to act
globally as a memristor.  This effect has also been shown in simulations of
small memristor networks in which a conducting backbone is formed through
the system.  We construct a mean field theory to describe the resistive
switching of the network and show a transition in the $ON/OFF$ ratio of
the memristors, below which no transition occurs.  The mean-field theory
also supplies a dynamical description of the system.  Using this, we examine
avalanches which occur in the course of the transition.  These are compared
to avalanches observed in neuronal cultures and used to derive distributions
of conductance jumps which serves as an analog of magnetic Barkhausen noise
in a conductive system undergoing memristive switching.

In order to highlight the general features of such networks, we take as
a concrete example the Atomic Switch Networks of Stieg et al. CITE.


\section{ATOMIC SWITCH NETWORKS}

$Ag | Ag_2 S | Ag$ gapless atomic switches consist of an insulating layer
of silver sulfide sandwiched between two silver electrodes.  As a voltage is
applied, the silver sulfide crystal undergoes a phase transition from
monoclinic acanthite to body centered argentite in which the diffusivity of
silver through the crystal increases dramatically.  As silver ions drift
from anode to cathode and deposit, a filament structure is formed, eventually
bridging the insulator \cite{Xu2010}. As the filament grows, the
tunneling gap shrinks and there is an eventual transition from tunneling
to ballistic conductance as the filament completes
\cite{Hasegawa2010, Sun2014}. Competition between the transport induced
filament growth and it's thermal fluctuation induced dissolution allow the
atomic switch to display learning abilities analogous to an organic synapse:
short current pulses through an incomplete filament will change the tunneling
gap, but these changes to the conductivity will be erased over short timescales
by thermal fluctuations, giving a short term memory behavior to the junction.
Longer current pulses will complete the filament, lengthening the timescales
for thermal dissolution and thus displaying a nonvolatile long term memory
\cite{Hasegawa2010, Ohno2011}.

$Ag_2 S$ atomic switches display bipolar resistive switching, meaning that
a resistance change can be reversed by reversing the polarity of the voltage.
This requires that the devices themselves are polar and typically this
polarity is imposed by the metallic configuration; devices are of the form
$Ag|Ag_2 S|W$ where $W$ is some inert metal that does not diffuse readily into
the crystal.  $Ag|Ag_2 S| Ag$ atomic switches are physically symmetric, yet
display the same bipolar switching.  We infer that their polarity is imposed
by the
'functionalization' step.  Gapless atomic switches require a forming step
to show bipolar resistive switching.  Voltage is applied from the $Ag$ side,
causing the formation of a stable filament across the crystal.  When the
voltage is reversed, the thinnest part of this filament undergoes a 
joule-heating assisted dissolution \cite{Hasegawa2012} and this smaller
junction in the filament is where subsequent switching will occur.  The device
thus has the additional asymmetry of the filament formation.  While we have
not seen this effect explored in detail in $Ag|Ag_2 S |Ag$ atomic switches,
we believe this to be a reasonable explanation for their induced polarity.

In order to achieve a complex disordered network architecture, Gimzewskii et
al. used an electroless deposition process in which solution phase $Ag^+$
exchanges with solid $Cu$.  By seeding an area with $Cu$ 
microspheres or seed posts, the depositing $Ag$ is led to form a
densely interconnected network of wires through diffusion-limited aggregation.
The structure of this network can be controlled through the copper
seeding and the concentration of the silver solution \cite{Avizienis2013},
but many dynamical behaviors appear to be present across different network
structures FIND CITATION.  Exposure of the resulting networks to sulfur gas
led to the formation of $Ag|Ag_2 S|Ag$ junctions throughout the network with
a density of $10^9$ switches/cm$^2$.  In order to functionalize the switches,
voltage sweeps are applied across the network until a sharp increase
in conductivity is observed.  After this, the network displays pinched
hystesis with an ON/OFF ration of $10^3$.

\section{MODEL \& METHODS}

We consider a memristive network under an adiabatically swept voltage (by
which we
mean that the timescale of voltage changes is much longer than the timescale
for an individual memristor to switch) and focus on the network transition
to it's conductive state.  For ideal memristors, any current
flowing through the network would eventually cause elements to progress to
their conducting state.  However, in the presence of a threshold the
conductivity will remain constant until the largest voltage drop in the
network has crossed it's threshold and it's conductivity begins to increase.
Increasing the
element's conductivity draws more current, both by diverting it from nearby
elements, and by increasing the conductivity of the network and drawing more
current at the boundaries.  This accelerates the switching process and causes
the element to switch to it's 'ON' or conductive state.  This switching
process,
through the two avenues just mentioned, increases the current through other
elements in the network and may lead to avalanches of memristive elements
switching from their 'OFF' insulating state to their 'ON' conducting state. 
The role of the structural disorder in the network is to induce a voltage
distribution across the elements which leads to their switching order. We also
expect the thresholds in a physical network to show some disorder due to
variations in the size of the elements. The switching order will be decided by some
combination of these, for example the ratio of the threshold voltage to the
voltage drop of each element.  This interplay of disorders allows us to
simplify our model by absorbing all contributing disorders into a single
distribution for the effective threshold of an element.  This assumption 


\section{SIMULATIONS}

Simulations were carried out for $p(t) = Uniform(0, 1)$ and a variety of
ON/OFF ratios. The network conductivities as a function of applied voltage
are displayed in Figure FIGURE. The lattice size is chosen so that the
network conductivity varies between $g_{off}$ and $g_{on}$ and has been
rescaled to accomodate the various plots.  We see that for small values
of $g_{on}/g_{off}$ the conductance is a smooth function of the voltage.
As we increase the ratio a discontinuity forms that at first appears
continuous, but as we increase the ratio further a jump in the
conductivity becomes evident, indicating a first order
phase transition. In the following we construct a mean field theory to
characterize this transition and from it clarify the roles of the disorder and
ON/OFF ratio.

\section{MEAN-FIELD THEORY}

Including disorder in a mean field theory, we arrive at a self consistency
equation similar to that of the Random Field Ising Model (RFIM) but with a
more complexFollowing the method of Zapperi et al. \cite{Zapperi1999} used in analyzing
random fuse networks,
we begin with the power dissipated by the network,
\[P = \sum_i \sigma_i \Delta V_i^2\]
where the sum runs over all elements in the network, $\sigma_i$ is the 
conductivity of element $i$  and the $\Delta V_i$ are assigned by minimizing
the total power dissipated by the network for given boundary conditions.
In order to construct a mean field theory, we aim to write the $\Delta V_i$ as
some function of the average conductivity $\phi = \frac{\sum_i \sigma_i}{N}$.
In this interest we  write the power dissipated in terms of the voltage at
the boundaries,
\[P = G(\{\sigma_i\}) V^2\]
where $G(\{\sigma_i\})$ is some function of the conductivity of each element
(for example their sum if all elements are connected in parallel). While we
do not in general know this function, we can obtain an approximate form
from effective medium theory $G_{eff}(f)$ (See Supplemental Material) as a
function of the fraction of
elements in the conducting state $f = n_{on}/N_{total}$. We can simply
exchange this for a dependence on the average
\(\phi = g_{off} + f(g_{on} - g_{off})\) to obtain
$P = G_{eff}(\phi) V^2$. Inserting factors of $\phi$ and pulling out the sum,
this yields
\[P = \sum_i \sigma_i \frac{G_{eff}(\phi)V^2}{\phi N}.\]
Thus, in the mean field theory, every element in the network is subject to
the same mean field voltage drop
\[\Delta V_{MF} = \sqrt{\frac{G_{eff}(\phi)}{\phi}}\frac{V}{\sqrt{N}}
= h(\phi) v.\]
We recognize $v=V/\sqrt{N}$ as the applied local field.  The function
$h(\phi)$ determines what fraction of the applied field each element
experiences at the mean field level.  In particular, if $h(f)$ is decreasing,
switching an element at a fixed voltage decreases $\Delta V_{MF}$ and
thus cannot cause other elements to switch.  If it is increasing, the
increased mean field voltage may cause other elements whose threshold have
been crossed to trip and precipitate an avalanche. From effective medium
theory,
\[G(f) = \frac{(2f-1)(g_{on} - g_{off}) + \sqrt{(2f-1)^2
(g_{on} - g_{off})^2+ 4g_{on}g_{off}}}{2}.\]
Using this to plot $h(f) = \sqrt{G(f)/\phi(f)}$ FIGURE we note both decreasing
and increasing intervals.  According to the mean field theory, this is due
to competition between $G(f)$, the network conductivity and $\phi(f)$ the
average conductivity of an element.  In regions where $\phi(f)$ is increasing
more quickly than $G(f)$, individual elements that switch are concentrating
current faster
than the network can increase the current at the boundaries, decreasing the
current through unswitched elements.  When $G(f)$ begins increasing more
quickly than $\phi(f)$, current is being drawn in at the boundaries faster
than it can be concentrated by switched elements and $\Delta V_{MF}$ increases,
allowing for avalanches and phase transitions. Here we see that our choice of
a voltage controlled network was not arbitrary.  A current controlled network
gives $h(f) = \frac{1}{\sqrt{G(f)\phi(f)}}$ which is decreasing for all $f$
as without
drawing more current at the boundaries, switching elements will always
concentrate current away from others. This analysis ignores the effects of
neighboring memristive elements however, and concentrating current may also
cause avalanches but only due to this spatial interaction.  We will return
to this point in the discussion.

In order to form a self consistency equation, we consider an arbirary element,
$\sigma_i$ with threshold $t_i$.  If the mean field voltage exceeds this
threshold, $sigma_i = g_{on}$. If not, it remains in it's OFF state. Across
elements in the network, the average will thus be,
\[\langle \sigma \rangle = g_{on} P(t < \Delta V_{MF}) + 
g_{off} P(t> \Delta V_{MF}.\]
This is conveniently recast in terms of $f$, and using the distribution
$p(t)$ as
\begin{equation}\label{selfconsist}
f = \int_0^{h(f) v} p(t) dt
\end{equation}
Here we can note that, as opposed to the Random Field Ising Model, the
dynamics of the
mean field theory will be independent of the scale of $p(t)$.  Because the
applied field enters multiplicatively, rescaling $t$ results in a factor that
is absorbed into $v$ such that we will see the same switching behavior but
at a stretched voltage scale. The LHS and
RHS are plotted for several values of the voltage in Figure FIGURE.  For
the chosen distribution and ON/OFF ratio a transition is evident at the
point
\begin{equation}\label{PT_eqn}
1 = p'(h(f)v)h'(f)v \quad 0\le f\le 1.
\end{equation}
If a particular disorder distribution and ON/OFF ratio allows for the
simultaneous solution of \ref{selfconsist} and \ref{PT_eqn}, the mean-field
theory demonstrates a phase transition.

\section{DISCUSSION}


Until recently, realizing systems with complex architecture
experimentally was an unmet challenge.  The advent of the memristor has
created opportunities for building electrical systems with complex, disordered
architecture and dynam

By taking advantage of a growth process, Gimzewskii et
al. CITE were able to fabricate disordered networks of memristive synapses
with a density of $10^8\; \text{synthetic synapses/cm}^2$.

The presence of disorder and robustness to noise in the brain
suggest a statistical character to the brain's function, at least at small
scales.  We put forth that one of the hopes of 'top-down' neuromorphic
engineering should be to 

The impending limits of our current computational paradigm have galvanized
interest in alternative forms of computation. As transistor densities have
increased, limits on transistor size and increased energy consumption have
spurred interest in electronic components that function reliably at small
scales and store their states passively.  Similarly, limits due to our
computational model such as the von Neumann Bottleneck have generated
interest in alternative schemes that do computation directly in memory.
CITE

Memristor based solutions have been proposed to meet both of these needs.
They are
necessarily nanoscale devices that passively maintain their resistance
state and can function both as logical and memory components.  As such,
several emerging technologies have centered around the memristor as
their core computational unit.  Among these are the crossbar array,
memristor neural networks and associative memories, memristive cellular
automata, and memcomputing.  While the conceptual and engineering
challenges to realizing new computational paradigms are great, we are
buoyed by the fact that the nervous system appears to perform computation
in just this way, with no discernable separation between memory and
logical faculties.  It would seem that memories are stored in a
distributed manner, within the synaptic connections of many neurons
and computation occurs through the generation of action potentials that
propogate through the system, modifying synaptic coupling globally.
Memristors' ability to mimic the learning properties of synapses such as
short-term and long-term memory, have further propelled their standing as
a possible hardware element for neuromorphic computing.

While most neuromorphic computing solutions have advocated a bottom-up
approach, designing fundamental units such as CMOS neurons
and linking them together in a
particular configuration to solve problems, a few have advocated a top
down approach.  In such an approach, the gross features of nervous
tissue are duplicated: a disordered, recurrent network with memory, and
the resulting system is studied to see what behaviors emerge.  As disorder
and noise play a strong role in the nervous system, we suspect a statistical
character to the dynamics and as such we investigate the statistical
dynamics of 



% If in two-column mode, this environment will change to single-column
% format so that long equations can be displayed. Use
% sparingly.
%\begin{widetext}
% put long equation here
%\end{widetext}

% figures should be put into the text as floats.
% Use the graphics or graphicx packages (distributed with LaTeX2e)
% and the \includegraphics macro defined in those packages.
% See the LaTeX Graphics Companion by Michel Goosens, Sebastian Rahtz,
% and Frank Mittelbach for instance.
%
% Here is an example of the general form of a figure:
% Fill in the caption in the braces of the \caption{} command. Put the label
% that you will use with \ref{} command in the braces of the \label{} command.
% Use the figure* environment if the figure should span across the
% entire page. There is no need to do explicit centering.

% \begin{figure}
% \includegraphics{}%
% \caption{\label{}}
% \end{figure}

% Surround figure environment with turnpage environment for landscape
% figure
% \begin{turnpage}
% \begin{figure}
% \includegraphics{}%
% \caption{\label{}}
% \end{figure}
% \end{turnpage}

% tables should appear as floats within the text
%
% Here is an example of the general form of a table:
% Fill in the caption in the braces of the \caption{} command. Put the label
% that you will use with \ref{} command in the braces of the \label{} command.
% Insert the column specifiers (l, r, c, d, etc.) in the empty braces of the
% \begin{tabular}{} command.
% The ruledtabular enviroment adds doubled rules to table and sets a
% reasonable default table settings.
% Use the table* environment to get a full-width table in two-column
% Add \usepackage{longtable} and the longtable (or longtable*}
% environment for nicely formatted long tables. Or use the the [H]
% placement option to break a long table (with less control than 
% in longtable).
% \begin{table}%[H] add [H] placement to break table across pages
% \caption{\label{}}
% \begin{ruledtabular}
% \begin{tabular}{}
% Lines of table here ending with \\
% \end{tabular}
% \end{ruledtabular}
% \end{table}

% Surround table environment with turnpage environment for landscape
% table
% \begin{turnpage}
% \begin{table}
% \caption{\label{}}
% \begin{ruledtabular}
% \begin{tabular}{}
% \end{tabular}
% \end{ruledtabular}
% \end{table}
% \end{turnpage}

% Specify following sections are appendices. Use \appendix* if there
% only one appendix.
%\appendix
%\section{}

% If you have acknowledgments, this puts in the proper section head.
%\begin{acknowledgments}
% put your acknowledgments here.
%\end{acknowledgments}

% Create the reference section using BibTeX:
%\bibliography{/home/forrest/Documents/BibTeX/Advancement_PT_Memnets}
\bibliography{/home/fsheldon/Documents/BibTeX/Advancement_PT_Memnets}

\end{document}
%
% ****** End of file apstemplate.tex ******

